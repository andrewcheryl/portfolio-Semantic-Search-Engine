<!DOCTYPE html><html><head><meta charset="utf-8"><style>@font-face {
  font-family: octicons-anchor;
  src: url(data:font/woff;charset=utf-8;base64,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) format('woff');
}

* {
    box-sizing: border-box;
}

body {
    width: 980px;
    margin-right: auto;
    margin-left: auto;
}

body .markdown-body {
    padding: 45px;
    border: 1px solid #ddd;
    border-radius: 3px;
    word-wrap: break-word;
}

pre {
    font: 12px Consolas, "Liberation Mono", Menlo, Courier, monospace;
}

.markdown-body {
  -webkit-text-size-adjust: 100%;
  text-size-adjust: 100%;
  color: #333;
  font-family: "Helvetica Neue", Helvetica, "Segoe UI", Arial, freesans, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
  font-size: 16px;
  line-height: 1.6;
  word-wrap: break-word;
}

.markdown-body a {
  background-color: transparent;
}

.markdown-body a:active,
.markdown-body a:hover {
  outline: 0;
}

.markdown-body strong {
  font-weight: bold;
}

.markdown-body h1 {
  font-size: 2em;
  margin: 0.67em 0;
}

.markdown-body img {
  border: 0;
}

.markdown-body hr {
  box-sizing: content-box;
  height: 0;
}

.markdown-body pre {
  overflow: auto;
}

.markdown-body code,
.markdown-body kbd,
.markdown-body pre {
  font-family: monospace, monospace;
  font-size: 1em;
}

.markdown-body input {
  color: inherit;
  font: inherit;
  margin: 0;
}

.markdown-body html input[disabled] {
  cursor: default;
}

.markdown-body input {
  line-height: normal;
}

.markdown-body input[type="checkbox"] {
  box-sizing: border-box;
  padding: 0;
}

.markdown-body table {
  border-collapse: collapse;
  border-spacing: 0;
}

.markdown-body td,
.markdown-body th {
  padding: 0;
}

.markdown-body input {
  font: 13px / 1.4 Helvetica, arial, nimbussansl, liberationsans, freesans, clean, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
}

.markdown-body a {
  color: #4078c0;
  text-decoration: none;
}

.markdown-body a:hover,
.markdown-body a:active {
  text-decoration: underline;
}

.markdown-body hr {
  height: 0;
  margin: 15px 0;
  overflow: hidden;
  background: transparent;
  border: 0;
  border-bottom: 1px solid #ddd;
}

.markdown-body hr:before {
  display: table;
  content: "";
}

.markdown-body hr:after {
  display: table;
  clear: both;
  content: "";
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  margin-top: 15px;
  margin-bottom: 15px;
  line-height: 1.1;
}

.markdown-body h1 {
  font-size: 30px;
}

.markdown-body h2 {
  font-size: 21px;
}

.markdown-body h3 {
  font-size: 16px;
}

.markdown-body h4 {
  font-size: 14px;
}

.markdown-body h5 {
  font-size: 12px;
}

.markdown-body h6 {
  font-size: 11px;
}

.markdown-body blockquote {
  margin: 0;
}

.markdown-body ul,
.markdown-body ol {
  padding: 0;
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body ol ol,
.markdown-body ul ol {
  list-style-type: lower-roman;
}

.markdown-body ul ul ol,
.markdown-body ul ol ol,
.markdown-body ol ul ol,
.markdown-body ol ol ol {
  list-style-type: lower-alpha;
}

.markdown-body dd {
  margin-left: 0;
}

.markdown-body code {
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  font-size: 12px;
}

.markdown-body pre {
  margin-top: 0;
  margin-bottom: 0;
  font: 12px Consolas, "Liberation Mono", Menlo, Courier, monospace;
}

.markdown-body .select::-ms-expand {
  opacity: 0;
}

.markdown-body .octicon {
  font: normal normal normal 16px/1 octicons-anchor;
  display: inline-block;
  text-decoration: none;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

.markdown-body .octicon-link:before {
  content: '\f05c';
}

.markdown-body:before {
  display: table;
  content: "";
}

.markdown-body:after {
  display: table;
  clear: both;
  content: "";
}

.markdown-body>*:first-child {
  margin-top: 0 !important;
}

.markdown-body>*:last-child {
  margin-bottom: 0 !important;
}

.markdown-body a:not([href]) {
  color: inherit;
  text-decoration: none;
}

.markdown-body .anchor {
  display: inline-block;
  padding-right: 2px;
  margin-left: -18px;
}

.markdown-body .anchor:focus {
  outline: none;
}

.markdown-body h1,
.markdown-body h2,
.markdown-body h3,
.markdown-body h4,
.markdown-body h5,
.markdown-body h6 {
  margin-top: 1em;
  margin-bottom: 16px;
  font-weight: bold;
  line-height: 1.4;
}

.markdown-body h1 .octicon-link,
.markdown-body h2 .octicon-link,
.markdown-body h3 .octicon-link,
.markdown-body h4 .octicon-link,
.markdown-body h5 .octicon-link,
.markdown-body h6 .octicon-link {
  color: #000;
  vertical-align: middle;
  visibility: hidden;
}

.markdown-body h1:hover .anchor,
.markdown-body h2:hover .anchor,
.markdown-body h3:hover .anchor,
.markdown-body h4:hover .anchor,
.markdown-body h5:hover .anchor,
.markdown-body h6:hover .anchor {
  text-decoration: none;
}

.markdown-body h1:hover .anchor .octicon-link,
.markdown-body h2:hover .anchor .octicon-link,
.markdown-body h3:hover .anchor .octicon-link,
.markdown-body h4:hover .anchor .octicon-link,
.markdown-body h5:hover .anchor .octicon-link,
.markdown-body h6:hover .anchor .octicon-link {
  visibility: visible;
}

.markdown-body h1 {
  padding-bottom: 0.3em;
  font-size: 2.25em;
  line-height: 1.2;
  border-bottom: 1px solid #eee;
}

.markdown-body h1 .anchor {
  line-height: 1;
}

.markdown-body h2 {
  padding-bottom: 0.3em;
  font-size: 1.75em;
  line-height: 1.225;
  border-bottom: 1px solid #eee;
}

.markdown-body h2 .anchor {
  line-height: 1;
}

.markdown-body h3 {
  font-size: 1.5em;
  line-height: 1.43;
}

.markdown-body h3 .anchor {
  line-height: 1.2;
}

.markdown-body h4 {
  font-size: 1.25em;
}

.markdown-body h4 .anchor {
  line-height: 1.2;
}

.markdown-body h5 {
  font-size: 1em;
}

.markdown-body h5 .anchor {
  line-height: 1.1;
}

.markdown-body h6 {
  font-size: 1em;
  color: #777;
}

.markdown-body h6 .anchor {
  line-height: 1.1;
}

.markdown-body p,
.markdown-body blockquote,
.markdown-body ul,
.markdown-body ol,
.markdown-body dl,
.markdown-body table,
.markdown-body pre {
  margin-top: 0;
  margin-bottom: 16px;
}

.markdown-body hr {
  height: 4px;
  padding: 0;
  margin: 16px 0;
  background-color: #e7e7e7;
  border: 0 none;
}

.markdown-body ul,
.markdown-body ol {
  padding-left: 2em;
}

.markdown-body ul ul,
.markdown-body ul ol,
.markdown-body ol ol,
.markdown-body ol ul {
  margin-top: 0;
  margin-bottom: 0;
}

.markdown-body li>p {
  margin-top: 16px;
}

.markdown-body dl {
  padding: 0;
}

.markdown-body dl dt {
  padding: 0;
  margin-top: 16px;
  font-size: 1em;
  font-style: italic;
  font-weight: bold;
}

.markdown-body dl dd {
  padding: 0 16px;
  margin-bottom: 16px;
}

.markdown-body blockquote {
  padding: 0 15px;
  color: #777;
  border-left: 4px solid #ddd;
}

.markdown-body blockquote>:first-child {
  margin-top: 0;
}

.markdown-body blockquote>:last-child {
  margin-bottom: 0;
}

.markdown-body table {
  display: block;
  width: 100%;
  overflow: auto;
  word-break: normal;
  word-break: keep-all;
}

.markdown-body table th {
  font-weight: bold;
}

.markdown-body table th,
.markdown-body table td {
  padding: 6px 13px;
  border: 1px solid #ddd;
}

.markdown-body table tr {
  background-color: #fff;
  border-top: 1px solid #ccc;
}

.markdown-body table tr:nth-child(2n) {
  background-color: #f8f8f8;
}

.markdown-body img {
  max-width: 100%;
  box-sizing: content-box;
  background-color: #fff;
}

.markdown-body code {
  padding: 0;
  padding-top: 0.2em;
  padding-bottom: 0.2em;
  margin: 0;
  font-size: 85%;
  background-color: rgba(0,0,0,0.04);
  border-radius: 3px;
}

.markdown-body code:before,
.markdown-body code:after {
  letter-spacing: -0.2em;
  content: "\00a0";
}

.markdown-body pre>code {
  padding: 0;
  margin: 0;
  font-size: 100%;
  word-break: normal;
  white-space: pre;
  background: transparent;
  border: 0;
}

.markdown-body .highlight {
  margin-bottom: 16px;
}

.markdown-body .highlight pre,
.markdown-body pre {
  padding: 16px;
  overflow: auto;
  font-size: 85%;
  line-height: 1.45;
  background-color: #f7f7f7;
  border-radius: 3px;
}

.markdown-body .highlight pre {
  margin-bottom: 0;
  word-break: normal;
}

.markdown-body pre {
  word-wrap: normal;
}

.markdown-body pre code {
  display: inline;
  max-width: initial;
  padding: 0;
  margin: 0;
  overflow: initial;
  line-height: inherit;
  word-wrap: normal;
  background-color: transparent;
  border: 0;
}

.markdown-body pre code:before,
.markdown-body pre code:after {
  content: normal;
}

.markdown-body kbd {
  display: inline-block;
  padding: 3px 5px;
  font-size: 11px;
  line-height: 10px;
  color: #555;
  vertical-align: middle;
  background-color: #fcfcfc;
  border: solid 1px #ccc;
  border-bottom-color: #bbb;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #bbb;
}

.markdown-body .pl-c {
  color: #969896;
}

.markdown-body .pl-c1,
.markdown-body .pl-s .pl-v {
  color: #0086b3;
}

.markdown-body .pl-e,
.markdown-body .pl-en {
  color: #795da3;
}

.markdown-body .pl-s .pl-s1,
.markdown-body .pl-smi {
  color: #333;
}

.markdown-body .pl-ent {
  color: #63a35c;
}

.markdown-body .pl-k {
  color: #a71d5d;
}

.markdown-body .pl-pds,
.markdown-body .pl-s,
.markdown-body .pl-s .pl-pse .pl-s1,
.markdown-body .pl-sr,
.markdown-body .pl-sr .pl-cce,
.markdown-body .pl-sr .pl-sra,
.markdown-body .pl-sr .pl-sre {
  color: #183691;
}

.markdown-body .pl-v {
  color: #ed6a43;
}

.markdown-body .pl-id {
  color: #b52a1d;
}

.markdown-body .pl-ii {
  background-color: #b52a1d;
  color: #f8f8f8;
}

.markdown-body .pl-sr .pl-cce {
  color: #63a35c;
  font-weight: bold;
}

.markdown-body .pl-ml {
  color: #693a17;
}

.markdown-body .pl-mh,
.markdown-body .pl-mh .pl-en,
.markdown-body .pl-ms {
  color: #1d3e81;
  font-weight: bold;
}

.markdown-body .pl-mq {
  color: #008080;
}

.markdown-body .pl-mi {
  color: #333;
  font-style: italic;
}

.markdown-body .pl-mb {
  color: #333;
  font-weight: bold;
}

.markdown-body .pl-md {
  background-color: #ffecec;
  color: #bd2c00;
}

.markdown-body .pl-mi1 {
  background-color: #eaffea;
  color: #55a532;
}

.markdown-body .pl-mdr {
  color: #795da3;
  font-weight: bold;
}

.markdown-body .pl-mo {
  color: #1d3e81;
}

.markdown-body kbd {
  display: inline-block;
  padding: 3px 5px;
  font: 11px Consolas, "Liberation Mono", Menlo, Courier, monospace;
  line-height: 10px;
  color: #555;
  vertical-align: middle;
  background-color: #fcfcfc;
  border: solid 1px #ccc;
  border-bottom-color: #bbb;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #bbb;
}

.markdown-body .plan-price-unit {
  color: #767676;
  font-weight: normal;
}

.markdown-body .task-list-item {
  list-style-type: none;
}

.markdown-body .task-list-item+.task-list-item {
  margin-top: 3px;
}

.markdown-body .task-list-item input {
  margin: 0 0.35em 0.25em -1.6em;
  vertical-align: middle;
}

.markdown-body .plan-choice {
  padding: 15px;
  padding-left: 40px;
  display: block;
  border: 1px solid #e0e0e0;
  position: relative;
  font-weight: normal;
  background-color: #fafafa;
}

.markdown-body .plan-choice.open {
  background-color: #fff;
}

.markdown-body .plan-choice.open .plan-choice-seat-breakdown {
  display: block;
}

.markdown-body .plan-choice-free {
  border-radius: 3px 3px 0 0;
}

.markdown-body .plan-choice-paid {
  border-radius: 0 0 3px 3px;
  border-top: 0;
  margin-bottom: 20px;
}

.markdown-body .plan-choice-radio {
  position: absolute;
  left: 15px;
  top: 18px;
}

.markdown-body .plan-choice-exp {
  color: #999;
  font-size: 12px;
  margin-top: 5px;
}

.markdown-body .plan-choice-seat-breakdown {
  margin-top: 10px;
  display: none;
}

.markdown-body :checked+.radio-label {
  z-index: 1;
  position: relative;
  border-color: #4078c0;
}
</style><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
MathJax.Hub.Config({
  config: ["MMLorHTML.js"],
  extensions: ["tex2jax.js"],
  jax: ["input/TeX"],
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    processEscapes: false
  },
  TeX: {
    extensions: ["AMSmath.js", "AMSsymbols.js"],
    TagSide: "right",
    TagIndent: ".8em",
    MultLineWidth: "85%",
    equationNumbers: {
      autoNumber: "AMS",
    },
    unicode: {
      fonts: "STIXGeneral,'Arial Unicode MS'"
    }
  },
  showProcessingMessages: false
});
</script>
<title>README</title></head><body><article class="markdown-body"><h1>
<a id="user-content-portfolio-project-novel-semantic-search-engine-for-wikipedia-articles" class="anchor" href="#portfolio-project-novel-semantic-search-engine-for-wikipedia-articles" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Portfolio project: Novel Semantic Search Engine for Wikipedia Articles</h1>
<h2>
<a id="user-content-the-problem-statement" class="anchor" href="#the-problem-statement" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>The Problem Statement</h2>
<p>Today there is a vast amount of digital information available and search engines are an essential tool for navigating the information space. In particular, finding 'relevant' or 'related' information with regards to a specific topic can be extremely challenging.</p>
<p>To improve search accuracy a 'Semantic' search engine seeks to understand the searcher's intent and the contextual meaning of terms as they appear in the searchable dataspace. These types of search engines aim to consider various points including context of search, location, intent, variation of words, synonyms, generalized and specialized queries, concept matching and natural language queries to provide relevant search results.</p>
<p>For this project I will focus on developing a natural language query tool to find related articles on Wikipedia.</p>
<h2>
<a id="user-content-goal" class="anchor" href="#goal" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Goal</h2>
<ol>
<li>Extract Wikipedia articles for a named Category.</li>
<li>Engineer a novel Wikipedia search engine.</li>
</ol>
<p>This problem was broken down into the following elements...</p>
<ul>
<li>Data collection process</li>
<li>Data Storage</li>
<li>Search algorithm development</li>
<li>User Interface</li>
</ul>
<h2>
<a id="user-content-highlevel-approach" class="anchor" href="#highlevel-approach" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Highlevel approach</h2>
<h3>
<a id="user-content-data-collection-and-storage" class="anchor" href="#data-collection-and-storage" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Data Collection and Storage</h3>
<p>The data collection process will be designed to capture all articles included within a specified category.</p>
<p>The Wiki API will be used to extract all pages contained within the category and any of its subcategories. The article for each page will then be extracted, cleaned and loaded into an NoSQL database that can be queried using the search engine.</p>
<h3>
<a id="user-content-search-algorithm" class="anchor" href="#search-algorithm" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Search Algorithm</h3>
<p>Latent semantic analysis (LSA) will be used to develop the search algorithm. This is a natural language processing technique for analyzing relationships between a set of documents and the terms they contain. LSA assumes that words that are close in meaning will occur in similar pieces of text (a distributional hypothesis).</p>
<p>3 key steps to creating the search engine:</p>
<ol>
<li>Construct an occurrence matrix from the corpus of documents (referred to as a document term matrix or dtm) containing word counts per document. The matrix uses rows to represent unique words and columns to represent each document. It is constructed as a sparse matrix. The elements of the matrix are weighted, using Tf-idf, to reflect the importance of the words in the corpus.</li>
</ol>
<p>Tf-idf stands for term frequency-inverse document frequency. It is is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The weight of a term that occurs in a document is simply proportional to the term frequency.</p>
<ol start="2">
<li>
<p>Apply Latent semantic analysis (LSA) - The document term matrix has very high dimensionality, so a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns. Specifically, we will use scikit learns TruncatedSVD algorithm that only computes the 'k' largest singular values. The resulting LSA matrix will use rows to represent the documents and columns to represent the 'components' of the lower dimensional space.</p>
</li>
<li>
<p>The similarity between two documents in the LSA matrix is measured by taking the cosine of the angle between the two vectors (or the dot product between the normalizations of the two vectors) formed by the two rows for the specified documents. Values close to 1 represent very similar components while values close to 0 represent very dissimilar components.</p>
</li>
</ol>
<p>Note:  To analyze a search term it must be included in the LSA matrix. This requires that each search term is appended to the document term matrix and the LSA matrix recalculated.</p>
<h2>
<a id="user-content-project-scope-and-structure" class="anchor" href="#project-scope-and-structure" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Project scope and structure</h2>
<ol>
<li>EDA</li>
<li>Database set up</li>
<li>Model Development
<ul>
<li>extraction process</li>
<li>search engine</li>
</ul>
</li>
<li>Test search engine performance</li>
<li>Model Build</li>
</ol>
<h2>
<a id="user-content-1-eda" class="anchor" href="#1-eda" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>1. EDA</h2>
<h3>
<a id="user-content-scope-of-data-to-be-included-in-search-engine" class="anchor" href="#scope-of-data-to-be-included-in-search-engine" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Scope of data to be included in search engine</h3>
<p>All sub-categories, pages and articles belonging to the following Wikipedia categories:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Category:Machine_learning" rel="nofollow">Machine Learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Category:Business_software" rel="nofollow">Business Software</a></li>
</ul>
<p>The raw page text and its category information will be written to a collection on a Mongo DB server running on a dedicated AWS instance.</p>
<p>A Mongo DB was chosen because it holds information in a JSON like format that is easy to work with using a python dictionary.</p>
<p>Code should be able to pick up any additional categories if requested.</p>
<p>The Wiki API will be used to extract Wikipedia content.</p>
<h3>
<a id="user-content-explore-structure-of-wiki-data" class="anchor" href="#explore-structure-of-wiki-data" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Explore structure of wiki data</h3>
<p>The Wikipedia data structure ...</p>
<pre><code> Category (Xp , Xc)
   Pages 
   Categories (Xp , Xc)
       Pages
       Categories (Xp,Xc)
          .....
</code></pre>
<p>where Xp = no. of pages , Xc = no. of subcategories</p>
<p>Observations:</p>
<ul>
<li>There is no maximum no. of levels.</li>
<li>Subcategories can belong to multiple categories/subcategories.</li>
<li>Subcategories can exist in multiple places within one category hierarchy.</li>
<li>Each page can be assigned to multiple categories/ subcategories and multiple times within one category hierarchy.</li>
</ul>
<p>Implication: There is a large amount of duplication, extremely deep nesting of subcategories is possible.</p>
<p>Model Requirements:</p>
<ul>
<li>Limit depth of search - default 2 levels of nested subcategories but allow this to be changed</li>
<li>Remove duplication of subcategories before pulling pages</li>
<li>Avoid duplication of pages</li>
<li>Assess no. of pages before triggering load of articles and provide option to abort (this can easily exceed 10,000 pages with load times &gt; an hour)</li>
</ul>
<h3>
<a id="user-content-explore-page-content" class="anchor" href="#explore-page-content" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Explore page content</h3>
<p>Pages have multiple elements. We are interested in article text and which categories pages belong too. To limit information pulled restrict wiki api queries to 'extract' and 'categories'.</p>
<p>Extracts are delivered inside a nested dictionary structure with html formatting.</p>
<p>Cleaning steps:</p>
<ol>
<li>Remove extract from dictionary</li>
<li>Use python 'beautifulsoup' package to parse html</li>
<li>Clean data - remove residual html, formulae, digits and non 'Latin' characters</li>
<li>Reduce dimensionality - lemmatize and remove stop words</li>
</ol>
<h2>
<a id="user-content-2-database-setup" class="anchor" href="#2-database-setup" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>2. Database setup</h2>
<p>The Wiki data will be stored in a Mongo Database on a separate AWS instance.</p>
<h3>
<a id="user-content-prepare-aws-server" class="anchor" href="#prepare-aws-server" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Prepare AWS server</h3>
<ol>
<li>Create EC2 t2.micro instance required with Ubuntu 16.04 image</li>
<li>Set up security group and open ports ...</li>
</ol>
<ul>
<li>22  - to accept SSH traffic</li>
<li>2376 - to accept inbound traffic from anywhere, used to pull images from Docker Hub</li>
<li>27016  - to accept inbound traffic from anywhere, used to connect with MongoDB</li>
</ul>
<ol start="3">
<li>Install Docker app</li>
<li>Pull Mongo docker image from Docker Hub</li>
<li>Create a new data volume for the Mongo database store</li>
<li>Run docker image to create a docker container</li>
</ol>
<h3>
<a id="user-content-mongo-db-setup" class="anchor" href="#mongo-db-setup" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Mongo DB setup</h3>
<p>Jupyter notebook: 01-Make-database.ipynb</p>
<p>The database is named myWiki.</p>
<p>Information is held in 5 collections...</p>
<p><em>1.</em> <em>category___collection</em> : this holds all the subcategories that belong to the load category.</p>
<p><em>2.</em><em>pagetest___collection</em> : this will hold a small test set of pages that can be used in development process to test extraction and cleaning process.</p>
<p><em>3.</em><em>page___collection</em> : this holds all the unique pages that belong to any category that has been loaded.</p>
<p>For each page the collection record holds</p>
<ul>
<li>pageid</li>
<li>title</li>
<li>extract</li>
<li>list of categories that a page belongs to</li>
</ul>
<p><em>4.</em><em>loads___collection</em> : holds a record of which categories have been loaded and when ( this currently is updated only when subcategories are loaded, need to add confirmation that related pages where loaded)</p>
<p><em>5.</em><em>pageload___collection</em> : holds a record of which categories have been loaded and when ( this currently is updated only when subcategories are loaded, need to add confirmation that related pages where loaded)</p>
<p>Some additional items were added to the end of this jupyter notebook to enable the management of database content.</p>
<h2>
<a id="user-content-3-model-development" class="anchor" href="#3-model-development" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>3. Model Development</h2>
<h3>
<a id="user-content-load-process" class="anchor" href="#load-process" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Load process</h3>
<p>Jupyter notebooks:</p>
<ul>
<li>02-Develop-and-test-data-cleaning.ipynb</li>
<li>03-Develop-extraction-process.ipynb</li>
</ul>
<p>A small test category was used for the model development process. The full extract of the 2 categories results in 6000+ pages, this will only be loaded once the model is working correctly.</p>
<p>Based on findings from the exploration of the wiki data structure the category load process was split into 5 parts:</p>
<ol>
<li>Pull all subcategories using a recursive function call. A list of subcategories is kept and checked for any duplication before further action.</li>
<li>Write a record of loaded subcategories to the mongo db.</li>
<li>Identify unique pages for unique set of subcategories identified in step 1</li>
<li>For each unique page, pull extract and category information</li>
<li>Clean extract and load to mongodb.</li>
</ol>
<p>An iterative process was used to establish the data cleaning criteria. For the initial iterations the cleaned extract was reviewed, and the criteria tweaked. This was followed by the creation and review of the document term matrix which resulted in some further tweaks to the cleaning criteria.</p>
<p>A class, called 'WikiAPI', containing the load functions was built implementing the final model.</p>
<p>Functions contained in "WikiAPI':</p>
<pre><code>'wiki_cats'  -&gt; calls 'pull_wiki_data' -&gt; calls 'read_categories' -&gt; 
returns unique list of subcategories &amp; ids,
writes subcategories to self.subcategories ready to load

'write_subcats_to_mongo' -&gt; writes contents of self.subcats to 
category collection in mongodb

'wiki_pages' -&gt; calls 'pull_wiki_data' - &gt; calls 'read_pages' -&gt; 
returns unique no. pages &amp; no. duplicates,
results written to self.pages ready for 'load_articles'

'load_articles' -&gt; calls 'read_articles' -&gt; calls 'pull_wiki_page' &amp; 'cleaner'
</code></pre>
<h3>
<a id="user-content-search-algorithm-development" class="anchor" href="#search-algorithm-development" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Search algorithm development</h3>
<p>Jupyter notebook: 04-Develop-Semantic-search-process.ipynb</p>
<p>The development of the search process was broken down into four steps:</p>
<ol>
<li>
<p>Build a 'corpus' of articles - during development this was limited to a test set of 255 pages so that the document term matrix (dtm) was small enough to load into a dataframe for inspection.</p>
</li>
<li>
<p>Construct the document term matrix for the corpus using Tfidf and load into a panads dataframe. (In the final model implentation the data is kept in a sparse matrix until the final step to present search results.)</p>
</li>
<li>
<p>Apply TruncatedSVD with 200 components. In early tests the size of the dtm dataframe caused lots of crashes so the development process was restricted to a smaller test set.</p>
</li>
<li>
<p>Build search function</p>
<ul>
<li>add search term to document term matrix (DTM) using Tfidf model fitted in step 2.</li>
<li>re-run Truncated SVD on augmented DTM</li>
<li>run cosine similarity to generate top 10 related aticles</li>
</ul>
</li>
</ol>
<p>The code for final model build wrapped in a Class.</p>
<p>Class Wiki_search - functions:</p>
<p>Document term matrix  created when class instantiated.
Search_mywiki can then be rerun multiple times
functions:</p>
<pre><code>__init__ -&gt; calls build_dtm_matrix -&gt; calls build_corpus -&gt; returns dtm sparse matrix , dtm index and fitted tfidf

 search_myWiki -&gt; returns  top 10 related articles
</code></pre>
<h3>
<a id="user-content-python-package-requirements" class="anchor" href="#python-package-requirements" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Python package requirements</h3>
<p>To run in a standard jupyter/scipy docker container  the following packages are required...</p>
<pre><code>- spacy
- nltk
- pymongo
- beautifulsoup 
</code></pre>
<p>plus download spacy.en and nltk all</p>
<p>All required packages can be installed by running the '00-Installed-Packages.ipynb' notebook.</p>
<h2>
<a id="user-content-4-test-search-engine-effectiveness" class="anchor" href="#4-test-search-engine-effectiveness" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>4. Test search engine effectiveness</h2>
<p>Evaluating search engine results is challenging, with limited resouces I choose to use a well known and trusted search engine to provide bench mark results.
I measured my search engine against the results found using google, with the search restricited to the en.wikipedia site.</p>
<p>Google search can be restricted to a particular site using ...</p>
<ul>
<li>' site:en.wikipedia.org SEARCHTERM '</li>
</ul>
<p>Comparing the top 10 results (where available from google) I tested..</p>
<ol>
<li>The overall effectiveness of my search engine</li>
<li>3 different sets of hyperparatmeters for creating the document term matrix using the Tfidf algorithm.</li>
</ol>
<p>On average my search engine found 3 of the top 5 results from google. This was a good performance given the google search was over the entire en.wikipedia content. Many of the items not picked up where not included in the MongoDB content.</p>
<p><a href="/Users/cherylandrew/Documents/dsi_plus/Projects/Project-4-semantic_search/search_engine_test.png" target="_blank"><img src="/Users/cherylandrew/Documents/dsi_plus/Projects/Project-4-semantic_search/search_engine_test.png" alt="" style="max-width:100%;"></a></p>
<p>The full results are in provided in the excel workbook  search-engine-tests.xlsx</p>
<h2>
<a id="user-content-5-model-build" class="anchor" href="#5-model-build" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>5. Model Build</h2>
<ul>
<li>Jupyter notebook : 05-Model-Build-Extraction-and-Semantic-search.ipynb</li>
<li>Python code  : wiki_api.py</li>
</ul>
<p>The code was designed so that it could be held in a single python file to enable deployment using a 'Flask micro web framework'. However , the category extraction process can result in a large number of pages being pulled and can take hours to complete. In order to provide feedback duing the extraction process, and an option to abort the load process, the front end was built in a jupyter notebook instead.</p>
<p>The code for the final model is wrapped into 3 Classes...</p>
<p>1.Class MyWikiDB - creates connection to MongoDb when required.</p>
<p>2.Class WikiAPI - load categories , this is instantiated with the category name, there is a run flag that defaults True. When the run flag is true it will trigger the full end to end process. If the run flag is false the steps can be triggered individually - this was added for debugging and exploration.</p>
<p>3.Class Wiki_Search - search enging , this require two steps...</p>
<ul>
<li>Document term matrix  created when class instantiated.</li>
<li>Search_mywiki can then be re-run multiple times</li>
</ul>
<p>The document term matrix is created and kept in a sparse matrix, if takes about 45 seconds to train when populated with the full 5800 pages but only needs to be trained once per 'search' session.</p>
</article></body></html>